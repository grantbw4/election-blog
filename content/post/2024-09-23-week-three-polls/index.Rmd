---
title: "Week Three - Polls"
author: "Grant Williams"
date: "2024-09-23"
output: pdf_document
categories: []
tags: []
slug: "week-three-polls"
---

# Introduction

In this third blog post, I want to explore **how we can *best* use polls to predict election outcomes.** I will do this through the following steps:

## 1) First, I am going to visualize poll accuracy over time.

## 2) Next, I will use various *regularization techniques* to create a model that will predict the 2024 election.

## 3) Then, I will use *ensemble learning* methods to combine models that incorporate both fundamental (economic) and polling data.

## 4) Lastly, I will discuss G. Elliott Morris and Nate Silver's *contrasting* approaches when it comes to weighting fundamental and poll-based forecasts.

The code used to produce these visualizations is publicly available in my [github repository](https://github.com/grantbw4/election-blog) and draws heavily from the section notes and sample code provided by the Gov 1347 Head Teaching Fellow, [Matthew Dardet](https://www.matthewdardet.com/harvard-election-analytics-2024/).

# Analysis 

```{r, include = FALSE, warning = FALSE, message = FALSE}
# Load libraries.
## install via `install.packages("name")`
library(car)
library(caret)
library(CVXR)
library(glmnet)
library(tidyverse)
library(gridExtra)

####----------------------------------------------------------#
#### Read, merge, and process data.
####----------------------------------------------------------#

# Read data (processed FiveThirtyEight polling average datasets).
d_pollav_natl <- read_csv("national_polls_1968-2024.csv")
```

First, I want to visualize polling data across years and identify whether there is a pattern for which weeks before the election (30 weeks before the election, 29 weeks before the election, 1 week before the election, etc.) are most predictive of the ultimate two-party national popular vote share of the election. I will do this by using a pre-processed, publicly available [FiveThirtyEight data set](https://projects.fivethirtyeight.com/polls/) of national polling averages since 1968.

I am choosing to look at 1988 because Michael Dukakis's ["death penalty" gaffe](https://content.time.com/time/specials/packages/article/0,28804,1844704_1844706_1844712,00.html) has often been cited as the death knell of Dukakis's campaign, and I am curious if this is reflected in the polling data. I am also going to investigate 2020 and 2024 as I am very familiar with the timelines of these elections. Given that I am trying to predict the 2024 election, it seems important that I am intimately familiar with the voting trends of 2020, as these trends are the most recent and could best help me predict the outcome of 2024. 

```{r, include = FALSE, warning = FALSE, message = FALSE}

####----------------------------------------------------------#
#### Visualizing poll variation over time.
####----------------------------------------------------------#

# Plot 3. Adding in Extra Dates of Interest for 2020 —— "game changers"? 
plot_2020 <- d_pollav_natl |> 
  filter(year == 2020) |> 
  ggplot(aes(x = poll_date, y = poll_support, color = party)) +
  geom_rect(xmin = as.Date("2020-08-17"), xmax = as.Date("2020-08-20"), ymin = 47.5, ymax = 100, alpha = 0.1, color = NA, fill = "grey") + 
  annotate("text", x = as.Date("2020-08-07"), y = 51.5, label = "DNC", size = 4) +
  geom_rect(xmin = as.Date("2020-08-24"), xmax = as.Date("2020-08-27"), ymin = 0, ymax = 47.2, alpha = 0.1, color = NA, fill = "grey") +
  annotate("text", x = as.Date("2020-09-04"), y = 45, label = "RNC", size = 4) +
  geom_rect(xmin = as.Date("2020-10-02"), xmax = as.Date("2020-10-12"), ymin = 0, ymax = 42.7, alpha = 0.05, color = NA, fill = "grey") +
  
  geom_point(size = 1) + 
  geom_line() + 
  
  geom_segment(x = as.Date("2020-03-12"), xend = as.Date("2020-03-12"), y = 0, yend = 44.8, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-03-12"), y = 42.5, label = "COVID \n Market Crash", size = 3) +
  geom_segment(x = as.Date("2020-04-08"), xend = as.Date("2020-04-08"), y = 49, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-03-25"), y = 51.3, label = "Bernie Ends Run", size = 3) +
  geom_segment(x = as.Date("2020-04-16"), xend = as.Date("2020-04-16"), y = 0, yend = 44, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-04-16"), y = 44.7, label = "22 mil \n Unemployment", size = 3) +
  geom_segment(x = as.Date("2020-05-27"), xend = as.Date("2020-05-27"), y = 0, yend = 43, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-06-05"), y = 44, label = "100k COVID Dead, \n George Floyd", size = 3) +
  
  geom_segment(x = as.Date("2020-07-14"), xend = as.Date("2020-07-14"), y = 0, yend = 50.3, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-06-19"), y = 47.5, label = "Moderna Announces", size = 3) +
  
  geom_segment(x = as.Date("2020-09-29"), xend = as.Date("2020-09-29"), y = 50, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-9-12"), y = 49.5, label = "Pres. Debate", size = 3) +
  geom_segment(x = as.Date("2020-10-07"), xend = as.Date("2020-10-07"), y = 51.7, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-10-17"), y = 50.3, label = "VP Debate", size = 3) +
  geom_segment(x = as.Date("2020-10-22"), xend = as.Date("2020-10-22"), y = 52, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-10-30"), y = 51.5, label = "Pres. Debate", size = 3) +
  annotate("text", x = as.Date("2020-10-15"), y = 43.7, label = "Trump Has COVID", size = 3) +
  geom_segment(x = as.Date("2020-09-18"), xend = as.Date("2020-09-18"), y = 50, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-09-03"), y = 51.5, label = "RBG Passes", size = 3) +
  
  scale_x_date(date_labels = "%b %d") + 
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
  labs(x = "Date",
       y = "Average Poll Approval", 
       title = "Polling Averages by Date, 2020 (with Game Changers?)") + 
  theme_classic()

```

```{r, include = FALSE, warning = FALSE, message = FALSE}

# Plot 4. Poll Averages and "Game Changers" for 1988
plot_1988 <- d_pollav_natl |>
  filter(year == 1988) |>
  ggplot(aes(x = poll_date, y = poll_support, color = party)) +
  geom_rect(xmin=as.Date("1988-07-18"), xmax=as.Date("1988-07-21"), ymin=47, ymax=100, alpha=0.1, colour=NA, fill="grey") +
  annotate("text", x=as.Date("1988-07-10"), y=50, label="DNC", size=4) +
  geom_rect(xmin=as.Date("1988-08-15"), xmax=as.Date("1988-08-18"), ymin=0, ymax=44, alpha=0.1, colour=NA, fill="grey") +
  annotate("text", x=as.Date("1988-08-26"), y=40, label="RNC", size=4) +
  
  geom_point(size = 1) +
  geom_line() + 
  
  geom_segment(x=as.Date("1988-09-13"), xend=as.Date("1988-09-13"), y=49, yend=100, lty=2, color="grey", alpha=0.4) +
  annotate("text", x=as.Date("1988-09-13"), y=52, label="Tank Gaffe\n(?)", size=3) +
  annotate("text", x=as.Date("1988-09-21"), y=57, label="Willie Horton Ad\n(?)", size=3) +
  geom_segment(x=as.Date("1988-09-21"), xend=as.Date("1988-09-21"), y=49, yend=100, lty=2, color="grey", alpha=0.4) +
  annotate("text", x=as.Date("1988-10-15"), y=64, label="Second Debate\n(Death\nPenalty\nGaffe)", size=3) +
  geom_segment(x=as.Date("1988-10-15"), xend=as.Date("1988-10-15"), y=49, yend=100, lty=2, color="grey", alpha=0.4) +
  scale_x_date(date_labels = "%b, %Y") +
  scale_color_manual(values = c("dodgerblue4","firebrick1")) +
  labs(x = "Date",
       y = "Average Poll Approval", 
       title = "Polling Averages by Date, 1988 (with Game Changers?)") + 
  theme_classic()

```

```{r, include = FALSE, warning = FALSE, message = FALSE}
# Plot 5. Poll Averages for 2024
plot_2024 <- d_pollav_natl |> 
  filter(year == 2024) |> 
  ggplot(aes(x = poll_date, y = poll_support, color = party)) +
  geom_point(size = 1) + 
  geom_line() + 
  scale_x_date(date_labels = "%b %d") + 
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
  labs(x = "Date",
       y = "Average Poll Approval", 
       title = "Polling Averages by Date, 2024") + 
  geom_segment(x=as.Date("2024-07-13"), xend=as.Date("2024-07-13"), y=42.3, yend=100, lty=2, color="grey", alpha=0.4) +
  annotate("text", x=as.Date("2024-06-15"), y=52, label="Trump 1st Assassination Attempt", size=3) +
  geom_segment(x=as.Date("2024-07-21"), xend=as.Date("2024-07-21"), y=40, yend=100, lty=2, color="grey", alpha=0.4) +
  annotate("text", x=as.Date("2024-08-03"), y=50, label="Biden drops out", size=3) +
  geom_rect(xmin=as.Date("2024-07-15"), xmax=as.Date("2024-07-18"), ymin=0, ymax=42.3, alpha=0.1, colour=NA, fill="grey") +
  annotate("text", x=as.Date("2024-07-26"), y=39.5, label="RNC", size=4) +
  geom_rect(xmin=as.Date("2024-08-19"), xmax=as.Date("2024-08-22"), ymin=46.7, ymax=100, alpha=0.1, colour=NA, fill="grey") +
  annotate("text", x=as.Date("2024-08-30"), y=51, label="DNC", size=4) +
  theme_classic()

```

```{r, echo = FALSE, warning = FALSE, message = FALSE}

# I used ChatGPT to make sure the plots lined up well

par(mfrow = c(3, 1),  # 3 rows, 1 column for the plots
    mar = c(4, 4, 2, 1),  # Margins for each plot (bottom, left, top, right)
    oma = c(0, 0, 2, 0),  # Outer margin for spacing
    pin = c(6, 18))  # Adjust the width and height of each plot to achieve the 3:1 ratio

# Plot each graph
plot(plot_1988)
plot(plot_2020)
plot(plot_2024)

# Reset plotting parameters if needed
par(mfrow = c(1, 1)) 
```

From these graphs, we can see several things.

Many of the events that pundits have characterized as determinative in elections have actually not been all that influential in candidates' polling averages. For example, in the 1988 election, neither [Michael Dukakis's tank ad gaffe](https://www.usnews.com/news/articles/2008/01/17/the-photo-op-that-tanked) nor George H.W. Bush's [infamous Willie Horton ad](https://www.nytimes.com/2018/12/03/us/politics/bush-willie-horton.html) had much of an effect on the polling average. The [death penalty gaffe](https://content.time.com/time/specials/packages/article/0,28804,1844704_1844706_1844712,00.html), however, did appear to offer Bush a sizeable bump in the election.

We can also see, in 2020, how the polls captured changes in voter sentiment following national emergencies and sizeable events like the COVID market crash and the George Floyd protests. Historically, each party's national conventions tend to boost the polling average of their respective candidate, but this effect has been more muted for the Democrats in recent elections.

I also find the polling averages since 2024 to be incredibly interesting, specifically the massive boost the Democratic party experienced after Biden dropped out of the presidential election and endorsed Kamala Harris.

In order to predict the 2024 election, I am going to construct a panel of the weekly two-party national popular vote polling averages in the 30 weeks leading up to the election between 1968 and 2020. I am then going to prepare a regression that assigns a coefficient to each of these weekly polling averages. This regression will then permit me to predict the 2024 election by plugging in this cycle's weekly polling averages so far.

In order to prevent overfitting, I will use three regularization techniques: ridge regression, LASSO regression, and elastic net regression. I will then compare these regularized approaches to the more basic OLS approach that we employed in week two.

Briefly stated, both ridge and LASSO regression employ a penalty term in the loss function that shrinks coefficients toward zero. Where OLS minimizes the sum of the squared residuals, Ridge also minimizes a penalty term that is proportional to the coefficients' squared value. LASSO does something very similar, except its penalty term is proportional to the coefficients' absolute value. This allows some coefficients to equal 0. Elastic Net combines both ridge and LASSO. These regularization techniques all mitigate the chance of overfitting because they preventing the calculation of large coefficients that fit statistical noise and do not consider the possibility of multicollinearity. 

```{r, echo = FALSE, warning = FALSE, message = FALSE}

####----------------------------------------------------------#
#### Regularized regression with polling data.
####----------------------------------------------------------#

# Read election results data. 
d_vote <- read_csv("popvote_1948-2020.csv")
d_vote$party[d_vote$party == "democrat"] <- "DEM"
d_vote$party[d_vote$party == "republican"] <- "REP"

# Shape and merge polling and election data using November polls. 
d_poll_nov <- d_vote |> 
  left_join(d_pollav_natl |> 
              group_by(year, party) |> 
              top_n(1, poll_date) |> 
              select(-candidate), 
            by = c("year", "party")) |> 
  rename(nov_poll = poll_support) |> 
  filter(year <= 2020) |> 
  drop_na()

# Create dataset of polling average by week until the election. 
d_poll_weeks <- d_pollav_natl |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |> 
  left_join(d_vote, by = c("year", "party"))
 
# Split into training and testing data based on inclusion or exclusion of 2024. 
d_poll_weeks_train <- d_poll_weeks |> 
  filter(year <= 2020)
d_poll_weeks_test <- d_poll_weeks |> 
  filter(year == 2024)

colnames(d_poll_weeks)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_train)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_test)[3:33] <- paste0("poll_weeks_left_", 0:30)


# Comparison of OLS and regularized regression methods. 
ols.pollweeks <- lm(paste0("pv2p ~ ", paste0( "poll_weeks_left_", 0:30, collapse = " + ")), 
                    data = d_poll_weeks_train)
#summary(ols.pollweeks) # N.B. Inestimable: p (31) > n (30)! 

# Separate data into X and Y for training. 
x.train <- d_poll_weeks_train |>
  ungroup() |> 
  select(all_of(starts_with("poll_weeks_left_"))) |> 
  as.matrix()
y.train <- d_poll_weeks_train$pv2p


# Ridge. 
ridge.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0) # Set ridge using alpha = 0. 

# Visualize Shrinkage
par(mar = c(5, 4, 4, 2) + 0.1)  
plot(ridge.pollsweeks, xvar = "lambda")
title(main = "Ridge Regression", line = 2.5)

# Get particular coefficients. 
#coef(ridge.pollsweeks, s = 0.1)

# Lasso.
lasso.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 1) # Set lasso using alpha = 1.

# Visualize shrinkage.
par(mar = c(5, 4, 4, 2) + 0.1)  
plot(lasso.pollsweeks, xvar = "lambda")
title(main = "Lasso Regression", line = 2.5)

# Get particular coefficients.
#coef(lasso.pollsweeks, s = 0.1)

# Elastic net.
enet.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0.5) # Set elastic net using alpha = 0.5.

# Visualize shrinkage.
par(mar = c(5, 4, 4, 2) + 0.1)  
plot(enet.pollsweeks, xvar = "lambda")
title(main = "Elastic Net Regression", line = 2.5)

```

From the plots above, we see that, with a larger penalty term hyperparameter, lambda, the regression coefficients shrink **toward** zero faster, in the case of ridge regression, and, in the case of LASSO regression, shrink **to** zero faster.

By using cross-validation to minimize mean squared error, we will find an optimally predictive value for the hyperparameter, lambda, for each of the three regularization techniques. Once we do so, we observe the following coefficient plot.

# Coefficient Plot

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Can use cross-validated versions to find the optimal values of lambda that minimize the MSE of your predictions. 
# N.B. Use set.seed() and your favorite number e.g., 12345, 02138, before each CV/any stochastic call if you want your results to be stable. 

set.seed(101)
cv.ridge.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 0)
cv.lasso.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 1)
cv.enet.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 0.5)

# Get minimum lambda values 
lambda.min.ridge <- cv.ridge.pollweeks$lambda.min
lambda.min.lasso <- cv.lasso.pollweeks$lambda.min
lambda.min.enet <- cv.enet.pollweeks$lambda.min

# Predict on training data using lambda values that minimize MSE.
mse.ridge <- mean((predict(ridge.pollsweeks, s = lambda.min.ridge, newx = x.train) - y.train)^2)
mse.lasso <- mean((predict(lasso.pollsweeks, s = lambda.min.lasso, newx = x.train) - y.train)^2)
mse.enet <- mean((predict(enet.pollsweeks, s = lambda.min.enet, newx = x.train) - y.train)^2)

# Generate plot comparing coefficients for each of the weeks. 
d.coefplot <- data.frame("OLS" = coef(ols.pollweeks)[-1], 
                         "Ridge" = coef(ridge.pollsweeks, s = lambda.min.ridge)[-1], 
                         "Lasso" = coef(lasso.pollsweeks, s = lambda.min.lasso)[-1], 
                         "Elastic Net" = coef(enet.pollsweeks, s = lambda.min.enet)[-1]) |> 
  rownames_to_column("coef_name") |> 
  pivot_longer(cols = -coef_name, names_to = "method", values_to = "coef_est") |> 
  mutate(week = rep(0:30, each = 4))

d.coefplot[which(is.na(d.coefplot$coef_est)),]$coef_est <- 0 

d.coefplot |>
  ggplot(aes(x = coef_est, y = reorder(coef_name, -week), color = method)) +
  geom_segment(aes(xend = 0, yend = reorder(coef_name, -week)), alpha = 0.5, lty = "dashed") +
  geom_vline(aes(xintercept = 0), lty = "dashed") +   
  geom_point() + 
  labs(x = "Coefficient Estimate", 
       y = "Coefficient Name", 
       title = "Comparison of Coefficients Across Regularization Methods") + 
  theme_classic()

```

As is clear in the coefficient plot, the use of regularization techniques dramatically reduces the magnitude of the regression coefficients, perhaps reflecting the pervasiveness of the multicollinearity. Ultimately, our model suggests that the polls from the week of an election are the most important, though weeks as far back as 25 or 26 weeks before the election can still be predictive.

Using the coefficients outputted from elastic-net regression, and inputting the polling data from 2024 thus far, we find that the Democratic nominee, Kamala Harris, is projected to walk away with 51.8% of the national two-party popular vote share. 

```{r, include = FALSE, warning = FALSE, message = FALSE}
# First check how many weeks of polling we have for 2024. 
d_pollav_natl |> 
  filter(year == 2024) |> 
  select(weeks_left) |> 
  distinct() |> 
  range() # Let's take week 30 - 7 as predictors since those are the weeks we have polling data for 2024 and historically. 

x.train <- d_poll_weeks_train |>
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", 7:30))) |> 
  as.matrix()
y.train <- d_poll_weeks_train$pv2p
x.test <- d_poll_weeks_test |>
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", 7:30))) |> 
  as.matrix()

# Using elastic-net for simplicity. 
set.seed(02138)
enet.poll <- cv.glmnet(x = x.train, y = y.train, alpha = 0.5)
lambda.min.enet.poll <- enet.poll$lambda.min

```

```{r, include = FALSE, warning = FALSE, message = FALSE}

# Predict 2024 Democratic national pv2p share using elastic-net. 
polls.pred <- predict(enet.poll, s = lambda.min.enet.poll, newx = x.test)

polls.pred
```
I am not wholly convinced, however, that using data from election cycles since 1968 to determine which polling weeks from 2024 will be most informative of 2024's ultimate outcome is the best way to predict the election. Many of the coefficient values could be products of the randomness of the timing of major campaign turning points rather than reflective of any underlying patterns. My instinct tells me that polling averages become more accurate as the election looms. For this reason, I want to explore some other techniques, like *ensembling* that other forecasters use.

Ensembling permits us to merge the predictions of multiple models

In this section, I will prepare two models to predict 2024's national two-party popular vote share:

1) An elastic net model using fundamental economic data from [the Federal Reserve Bank of St. Louis](https://fred.stlouisfed.org/).

2) An elastic net model that exclusively uses polling data.

I will then ensemble these models using three approaches: 

1) *Unweighted*: We average the predictions from the fundamental and polling models.

2) *Silver Weighting Approach:* The weights on the polling model increase as election day nears.

3) *Gelman & King Approach Weighting Approach:* The weights on the both the economic and polling model increase as election day nears.

The polling prediction of the Democratic candidate, Kamala Harris, is listed first.

```{r, include = FALSE, warning = FALSE, message = FALSE}

####----------------------------------------------------------#
#### Model ensembling.
####----------------------------------------------------------#

# Estimate models using polls alone, fundamentals alone, and combined fundamentals and polls. 
# Read economic data. 
d_econ <- read_csv("fred_econ.csv") |> 
  filter(quarter == 2)

# Combine datasets and create vote lags. 
d_combined <- d_econ |> 
  left_join(d_poll_weeks, by = "year") |> 
  filter(year %in% c(unique(d_vote$year), 2024)) |> 
  group_by(party) |> 
  mutate(pv2p_lag1 = lag(pv2p, 1), 
         pv2p_lag2 = lag(pv2p, 2)) |> 
  ungroup() |> 
  mutate(gdp_growth_x_incumbent = GDP_growth_quarterly * incumbent, 
         rdpi_growth_quarterly = RDPI_growth_quarterly * incumbent,
         cpi_x_incumbent = CPI * incumbent,
         unemployment_x_incumbent = unemployment * incumbent,
         sp500_x_incumbent = sp500_close * incumbent) # Generate interaction effects.

# Create fundamentals-only dataset and split into training and test sets. 
d_fund <- d_combined |> 
  select("year", "pv2p", "GDP", "GDP_growth_quarterly", "RDPI", "RDPI_growth_quarterly", "CPI", "unemployment", "sp500_close",
         "incumbent", "gdp_growth_x_incumbent", "rdpi_growth_quarterly", "cpi_x_incumbent", "unemployment_x_incumbent", "sp500_x_incumbent", 
         "pv2p_lag1", "pv2p_lag2") 
x.train.fund <- d_fund |> 
  filter(year <= 2020) |>
  select(-c(year, pv2p)) |> 
  slice(-c(1:9)) |> 
  as.matrix()
y.train.fund <- d_fund |> 
  filter(year <= 2020) |> 
  select(pv2p) |> 
  slice(-c(1:9)) |> 
  as.matrix()
x.test.fund <- d_fund |> 
  filter(year == 2024) |> 
  select(-c(year, pv2p)) |> 
  drop_na() |> 
  as.matrix()

# Estimate elastic-net using fundamental variables only.
set.seed(101)
enet.fund <- cv.glmnet(x = x.train.fund, y = y.train.fund, intercept = FALSE, alpha = 0.5)
lambda.min.enet.fund <- enet.fund$lambda.min
```

# Ensemble Comparison

```{r, include = FALSE, warning = FALSE, message = FALSE}
# Predict 2024 national pv2p share using elastic-net. 
(fund.pred <- predict(enet.fund, s = lambda.min.enet.fund, newx = x.test.fund))

```

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Sequester data for combined model.
d_combo <- d_combined |> 
  select("year", "pv2p", "GDP", "GDP_growth_quarterly", "RDPI", "RDPI_growth_quarterly", "CPI", "unemployment", "sp500_close",
         "incumbent", "gdp_growth_x_incumbent", "rdpi_growth_quarterly", "cpi_x_incumbent", "unemployment_x_incumbent", "sp500_x_incumbent", 
         "pv2p_lag1", "pv2p_lag2", all_of(paste0("poll_weeks_left_", 7:30))) 

x.train.combined <- d_combo |> 
  filter(year <= 2020) |> 
  select(-c(year, pv2p)) |> 
  slice(-c(1:9)) |> 
  as.matrix()
y.train.combined <- d_combo |>
  filter(year <= 2020) |> 
  select(pv2p) |> 
  slice(-c(1:9)) |> 
  as.matrix()
x.test.combined <- d_combo |>
  filter(year == 2024) |> 
  select(-c(year, pv2p)) |> 
  drop_na() |> 
  as.matrix()
  
# Estimate combined model.
set.seed(1)
enet.combined <- cv.glmnet(x = x.train.combined, y = y.train.combined, intercept = FALSE, alpha = 0.5)
lambda.min.enet.combined <- enet.combined$lambda.min

# Predict 2024 national pv2p share using elastic-net.
combo.pred <- predict(enet.combined, s = lambda.min.enet.combined, newx = x.test.combined)

# Ensemble 1: Predict based on unweighted (or equally weighted) ensemble model between polls and fundamentals models. 
unweighted.ensemble.pred <- (polls.pred + fund.pred)/2

# Ensemble 2: Weight based on polls mattering closer to November. (Nate Silver)
election_day_2024 <- "2024-11-05"
today <- "2024-09-18"
days_left <- as.numeric(as.Date(election_day_2024) - as.Date(today))

poll_model_weight <- 1- (1/sqrt(days_left))
fund_model_weight <- 1/sqrt(days_left)

ensemble.2.pred <- polls.pred * poll_model_weight + fund.pred * fund_model_weight

# Ensemble 3. Weight based on fundamentals mattering closer to November. (Gelman & King, 1993)
poll_model_weight <- 1/sqrt(days_left)
fund_model_weight <- 1-(1/sqrt(days_left))

ensemble.3.pred <- polls.pred * poll_model_weight + fund.pred * fund_model_weight

```

```{r, echo = FALSE, warning = FALSE, message = FALSE}

# I used ChatGPT to aid me in preparing this visual

# Create a data frame with the ensemble results
ensemble_data <- data.frame(
  Ensemble = rep(c("Ensemble 1", "Ensemble 2", "Ensemble 3"), each = 2),
  Value = c(51.51353, 49.14507, 
            51.71210, 50.22182, 
            51.31497, 48.06832),
  Type = rep(c("Dem", "Rep"), times = 3)
)

# Convert to long format if necessary (already in long format here)
ensemble_data_long <- pivot_longer(ensemble_data, cols = Value, names_to = "Metric", values_to = "Score")

# Create the plot
ggplot(ensemble_data, aes(x = Ensemble, y = Value, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("Dem" = "dodgerblue4", "Rep" = "firebrick1")) +
  labs(title = "Ensemble Results", x = "Ensemble", y = "Score") +
  theme_minimal()

```

Though the ensemble prediction does not add up perfectly to 100 since we are predicting both Democratic and Republican vote share separately, we can clearly see the trend that all three ensemble models project that Harris will be the next president.

## Other Methods

In recent years, two of the leading voices in election forecasting, Nate Silver and G. Elliott Morris, have publicly sparred on Twitter over their divergent approaches to weighting fundamentals and poll-based forecasts. 

# Silver's Approach

Nate Silver outlines his methodology on his site, the ["Silver Bulletin."](https://www.natesilver.net/p/model-methodology-2024)

In brief, Silver's model prioritizes real-time polls, considering them most predictive of the actual election (and adjusting for polling bias and down-ballot effects of other races). While he does consider economic predictors and political assets like incumbency, these are secondary in importance. His model also has no current assumption about turnout for Democrats versus Republicans.

# Morris's Approach

Morris explains his philosophy on ABC's election site, [FiveThirtyEight.](https://abcnews.go.com/538/538s-2024-presidential-election-forecast-works/story?id=110867585)

Morris's model shares many similar characteristics to Silver's like reduced assumptions about COVID restrictions and a large emphasis on real-time polling data. Some of the most significant departures, however, are Morris's more aggressive adjustment for conventions and his assumption about intra-region state correlation. Morris's model also weights economic and political fundamentals more heavily, including these covariates alongside polling data rather than secondarily as Silver does.

# My opinion

I personally tend to agree more with Nate Silver's model. My impression is that Morris bakes too many of his own hunches and beliefs into the election forecast. For example, the division of the United States into correlated regions seems incredibly arbitrary and baseless to me. Likewise, Morris believes very strongly in incumbency advantage and is fairly skeptical of the polls, and, for this reason, considers polling data and political and economic fundamentals alongside one another. 

I believe Nate Silver's approach to be much more scientific. It does not allow for as much uncertainty as Morris's and lets polls (adjusted for confidence and bias) speak for themselves as they primarily shape the model. Silver's model, in my mind, is more objective. To me, I don't see much need to assign much weight to electoral patterns and historic behavior when the connection between pre-election polls and actual voting outcomes are much more immediate.

# Citations:

Morris, G. E. (2024, June 11). *How 538’s 2024 presidential election forecast works*. ABC News. https://abcnews.go.com/538/538s-2024-presidential-election-forecast-works/story?id=110867585 

Silver, N. (2024, June 26). *2024 Presidential Election Model Methodology Update*. Silver Bulletin. https://www.natesilver.net/p/model-methodology-2024 

# Data Sources: 

Data are from the US presidential election popular vote results from 1948-2020, [FiveThirtyEight](https://projects.fivethirtyeight.com/polls/), and [the Federal Reserve Bank of St. Louis](https://fred.stlouisfed.org/).
